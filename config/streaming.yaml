dataset:
  tokenizer:
    _target_: ezspeech.modules.dataset.utils.text.Tokenizer
    spe_file: /home4/khanhnd/exported_checkpoint/checkpoint_streaming/tokenizer.model
  train_ds:
    _target_: ezspeech.modules.dataset.dataset.SpeechRecognitionDataset
    filepaths:
      - /home4/khanhnd/Ezspeech/data/libri.jsonl
    # augmentation:
          # speed_perturbation:
          #   _target_: ezspeech.datas.augment.SpeedPerturbation
          #   orig_freqs: 16000
          #   factors: [0.9, 1.1, 1.0, 1.0, 1.0]

          # pitchshift:
          #   _target_: ezspeech.datas.augment.PitchShift
          #   min_step: -5
          #   max_step: 5
          #   sample_rates: [16000]
          #   probability: 0.2

          # rir_noise:
          #   _target_: ezspeech.datas.augment.ApplyImpulseResponse
          #   rir_filepath_16k: /data/sondd9/noise-16k/RIR.json
          #   second_before_peak: 0.01
          #   second_after_peak: 0.5
          #   probability: 0.2

          # background_noise:
          #   _target_: ezspeech.augment.AddBackgroundNoise
          #   noise_filepath_16k: /data/sondd9/noise-16k/background-noise.json
          #   # noise_filepath_8k: /dataset/8k/noise/background-noise.json
          #   min_snr_db: 0.0
          #   max_snr_db: 30.0
          #   probability: 0.2



  val_ds:
    _target_: ezspeech.modules.dataset.dataset.SpeechRecognitionDataset
    filepaths:
    - /home4/khanhnd/Ezspeech/data/libri_test.jsonl

  loaders:
    batch_size: 2
    num_workers: 8
    pin_memory: false

model:
  # variables for TDT configs.
  tdt_durations: [0, 1, 2, 3, 4]
  num_tdt_durations: 5
  enc_hidden: &enc_hidden 512
  pred_hidden: &pred_hidden 640
  joint_hidden: &joint_hidden 640
  vocab_size: &vocab_size 1024
  init_from_nemo_model:
  model_pretrained:
    path: /home4/khanhnd/exported_checkpoint/checkpoint_streaming
    include: ["preprocessor","encoder",ctc_decoder]
  freeze_encoder_steps: 1000
  preprocessor:
    _target_: ezspeech.modules.dataset.utils.audio.AudioToMelSpectrogramPreprocessor
    sample_rate: 16000
    normalize: per_feature
    window_size: 0.025
    window_stride: 0.01
    window: hann
    features: 80
    n_fft: 512
    frame_splicing: 1
    dither: 1.0e-05
    pad_to: 0
  spec_augment:
    _target_: ezspeech.modules.dataset.augment.SpecAugmentNumba
    freq_masks: 2
    time_masks: 10
    freq_width: 27
    time_width: 0.05
  encoder:
    _target_: ezspeech.modules.encoder.conformer.ConformerEncoder
    feat_in: ${model.preprocessor.features}
    feat_out: -1 # you may set it if you need different output size other than the default d_model
    n_layers: 17
    d_model: 512
    use_bias: True # whether to apply bias in the feedforward, MHA and convolution modules

    # Sub-sampling parameters
    subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding
    subsampling_factor: 8 # must be power of 2 for striding and vggnet
    subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model
    causal_downsampling: true

    # Feed forward module's params
    ff_expansion_factor: 4

    # Multi-headed Attention Module's params
    self_attention_model: rel_pos # rel_pos or abs_pos
    n_heads: 8 # may need to be lower for smaller d_models

    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
    # for att_context_style=regular, the right context is recommended to be a small number around 0 to 3 as multiple-layers may increase the effective right context too large
    # for att_context_style=chunked_limited, the left context need to be dividable by the right context plus one
    # look-ahead(secs) = att_context_size[1]*subsampling_factor*window_stride, example: 13*8*0.01=1.04s

    # For multi-lookahead models, you may specify a list of context sizes. During the training, different context sizes would be used randomly with the distribution specified by att_context_probs.
    # The first item in the list would be the default during test/validation/inference.
    # An example of settings for multi-lookahead:
    att_context_size: [[70,13],[70,6],[70,1],[70,0]]
    att_context_probs: [0.25, 0.25, 0.25, 0.25]
    # att_context_size: [70, 13] # -1 means unlimited context
    att_context_style: chunked_limited # regular or chunked_limited
    # att_context_probs: null

    xscaling: true # scales up the input embeddings by sqrt(d_model)
    pos_emb_max_len: 5000

    # Convolution module's params
    conv_kernel_size: 9
    conv_norm_type: 'layer_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)

    # conv_context_size can be"causal" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size
    # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]
    # Recommend to use causal convolutions as it would increase the effective right context and therefore the look-ahead significantly
    conv_context_size: causal

    ### regularization
    dropout: 0.1 # The dropout used in most of the Conformer Modules
    dropout_pre_encoder: 0.1 # The dropout used before the encoder
    dropout_emb: 0.0 # The dropout used for embeddings
    dropout_att: 0.1 # The dropout for multi-headed attention modules

    # set to non-zero to enable stochastic depth
    stochastic_depth_drop_prob: 0.0
    stochastic_depth_mode: linear  # linear or uniform
    stochastic_depth_start_layer: 1
  ctc_decoder:
    _target_: ezspeech.modules.decoder.decoder.ConvASRDecoder
    feat_in: *enc_hidden
    num_classes: *vocab_size



  decoder:
    _target_: ezspeech.modules.decoder.rnnt.rnnt.RNNTDecoder
    # _target_: nemo.collections.asr.modules.rnnt.RNNTDecoder
    vocab_size: *vocab_size
    normalization_mode: null # Currently only null is supported for export.
    random_state_sampling: false # Random state sampling: https://arxiv.org/pdf/1910.11455.pdf
    blank_as_pad: true # This flag must be set in order to support exporting of RNNT models + efficient inference.
    prednet:
      pred_hidden: *pred_hidden
      pred_rnn_layers: 1
      t_max: null
      dropout: 0.2
        # this is used to set the blank index in the prediction net, which is required for TDT models

  joint:
    _target_: ezspeech.modules.decoder.rnnt.rnnt.RNNTJoint
    # _target_: nemo.collections.asr.modules.rnnt.RNNTJoint
    num_classes: *vocab_size
    log_softmax: null  # 'null' would set it automatically according to CPU/GPU device
    preserve_memory: false  # dramatically slows down training, but might preserve some memory

    # Fuses the computation of prediction net + joint net + loss + WER calculation
    # to be run on sub-batches of size `fused_batch_size`.
    # When this flag is set to true, consider the `batch_size` of *_ds to be just `encoder` batch size.
    # `fused_batch_size` is the actual batch size of the prediction net, joint net and transducer loss.
    # Using small values here will preserve a lot of memory during training, but will make training slower as well.
    # An optimal ratio of fused_batch_size : *_ds.batch_size is 1:1.
    # However, to preserve memory, this ratio can be 1:8 or even 1:16.
    # Extreme case of 1:B (i.e. fused_batch_size=1) should be avoided as training speed would be very slow.
    fuse_loss_wer: true
    fused_batch_size: 2

    jointnet:
      encoder_hidden: *enc_hidden
      pred_hidden: ${model.decoder.prednet.pred_hidden}
      joint_hidden: *joint_hidden
      activation: "relu"
      dropout: 0.2
    num_extra_outputs: ${model.num_tdt_durations}

    
    
loss:
  ctc_loss:
    _target_: ezspeech.modules.losses.ctc.CTCLoss
    blank_idx: *vocab_size
    reduction: mean
    zero_infinity: false
  rnnt_loss:
    _target_: ezspeech.modules.losses.rnnt.TDTLoss
    # _target_: nemo.collections.asr.parts.numba.rnnt_loss.rnnt_pytorch.TDTLossNumba
    blank_idx: *vocab_size
    reduction: mean
    # FastEmit regularization: https://arxiv.org/abs/2010.11148
    # You may enable FastEmit to reduce the latency of the model for streaming
    fastemit_lambda: 0.000  # Recommended values to be in range [1e-4, 1e-2], 0.001 is a good start.
    clamp: -1.0  # if > 0, applies gradient clamping in range [-clamp, clamp] for the joint tensor only.

    # refer to https://arxiv.org/abs/2304.06795 for the meaning of the following three configs.
    durations: ${model.tdt_durations}
    sigma: 0.02 # hyper-param for under-normalization.
    omega: 0.1 # weight for regular RNN-T loss.
metric:
  decoding: self.decoding,
  batch_dim_index: self.wer.batch_dim_index,
  use_cer: self.wer.use_cer,
  log_prediction: self.wer.log_prediction,
  dist_sync_on_step: True,

optimizer:
  lr: 1
  betas: [0.9, 0.999]
  weight_decay: 1e-2
  eps: 1e-9
  foreach: False
  fused: True

scheduler:
  d_model: *enc_hidden
  warmup_steps: 10000
  
callbacks:
  lr:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor

  # swa:
  #   _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
  #   swa_lrs: 1e-6
  #   swa_epoch_start: 0.8
  #   annealing_epochs: 1

  cb:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: step
    save_top_k: 5
    save_last: True
    filename: "{epoch}-{val_ctc_loss:.5f}-{step}"
    every_n_epochs: 1
    mode: max

loggers:
  tb:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ../lightning_logs/oov
    name: null
    version: testcode2

    default_hp_metric: false

trainer:
  max_epochs: 200
  strategy: ddp
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 1
  # gradient_clip_val: 1
  # detect_anomaly: True
  precision: bf16-mixed
  val_check_interval: 0.5