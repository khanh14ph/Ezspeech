# Test configuration for EzSpeech models
# This configuration includes model checkpoint and test file paths

# Model checkpoint to evaluate (REQUIRED)
model_checkpoint: /home3/khanhnd/exported_checkpoint/alternate.ckpt

# Evaluation settings
eval_batch_size: 8
eval_num_workers: 4
output_dir: outputs/test_evaluation

# Datasets to evaluate on
eval_datasets:
  librispeech_test:
    _target_: ezspeech.modules.data.dataset.SpeechRecognitionDataset
    filepaths:
      - /home3/khanhnd/download/test_libri_clean.jsonl
    data_dir: /home3/khanhnd/download/

  vietbud_test:
    _target_: ezspeech.modules.data.dataset.SpeechRecognitionDataset
    filepaths:
      - /home3/khanhnd/download/vietbud_test.jsonl
    data_dir: /home3/khanhnd/download/

# Individual files to evaluate (optional)
eval_files:
  - audio_path: /home3/khanhnd/download/sample_audio_1.wav
    reference_text: "this is a test sample"
  - audio_path: /home3/khanhnd/download/sample_audio_2.wav
    reference_text: "another test example"

# Metrics configuration
metrics:
  # Basic metrics (always computed)
  basic: true

  # Detailed error analysis
  error_analysis: true

  # Confidence-based metrics (if model provides confidence scores)
  confidence_metrics: false

  # Text normalization for evaluation
  normalize_text: true

# Dataset configuration (inherited from training config)
dataset:
  spe_file_grapheme: /home3/khanhnd/download/grapheme.model
  spe_file_phoneme: /home3/khanhnd/download/ipa.model

# Model configuration (subset needed for evaluation)
model:
  # Model dimension (embedding size)
  d_model: 512
  # Vocabulary size including blank token
  vocab_size: 2048
  vocab_size_phoneme: 2048

  preprocessor:
    _target_: ezspeech.modules.data.utils.audio.AudioToMelSpectrogramPreprocessor
    sample_rate: 16000
    normalize: per_feature
    window_size: 0.025
    window_stride: 0.01
    window: hann
    features: 80
    n_fft: 512
    frame_splicing: 1
    dither: 1.0e-05
    pad_to: 0

  encoder:
    _target_: ezspeech.modules.encoder.conformer_offline.ConformerOfflineEncoder
    feat_in: ${model.preprocessor.features}
    feat_out: -1
    n_layers: 12
    d_model: 512
    sc_layers: [2,4,6,8,10]
    sc_layers_ipa: [1,3,5,7,9]
    subsampling: dw_striding
    subsampling_factor: 8
    subsampling_conv_channels: 256
    causal_downsampling: false
    reduction: null
    reduction_position: null
    reduction_factor: 1
    ff_expansion_factor: 4
    self_attention_model: rel_pos
    n_heads: 8
    att_context_size: [-1, -1]
    att_context_style: regular
    xscaling: false
    untie_biases: true
    pos_emb_max_len: 5000
    conv_kernel_size: 9
    conv_norm_type: batch_norm
    conv_context_size: null
    dropout: 0.1
    dropout_pre_encoder: 0.1
    dropout_emb: 0.0
    dropout_att: 0.1

  ctc_decoder:
    _target_: ezspeech.modules.decoder.decoder.ConvASRDecoder
    feat_in: ${model.d_model}
    num_classes: ${model.vocab_size}
    add_blank: true

  ctc_decoder_phoneme:
    _target_: ezspeech.modules.decoder.decoder.ConvASRDecoder
    feat_in: ${model.d_model}
    num_classes: ${model.vocab_size_phoneme}
    add_blank: true