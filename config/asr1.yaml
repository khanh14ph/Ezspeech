task:
  _target_: ezspeech.tasks.recognition.SpeechRecognitionTask

  dataset:
    vocab: &vocab /home4/khanhnd/Ezspeech/ezspeech/resource/vocab/vi_en.txt
    train_ds:
      _target_: ezspeech.modules.dataset.dataset.SpeechRecognitionDataset
      filepaths:
        - /home4/khanhnd/Ezspeech/data/vn.jsonl
      vocab: *vocab
      augmentation:
        # audio_augment:
          # speed_perturbation:
          #   _target_: ezspeech.datas.augment.SpeedPerturbation
          #   orig_freqs: 16000
          #   factors: [0.9, 1.1, 1.0, 1.0, 1.0]

          # pitchshift:
          #   _target_: ezspeech.datas.augment.PitchShift
          #   min_step: -5
          #   max_step: 5
          #   sample_rates: [16000]
          #   probability: 0.2

          # rir_noise:
          #   _target_: ezspeech.datas.augment.ApplyImpulseResponse
          #   rir_filepath_16k: /data/sondd9/noise-16k/RIR.json
          #   second_before_peak: 0.01
          #   second_after_peak: 0.5
          #   probability: 0.2

          # background_noise:
          #   _target_: ezspeech.augment.AddBackgroundNoise
          #   noise_filepath_16k: /data/sondd9/noise-16k/background-noise.json
          #   # noise_filepath_8k: /dataset/8k/noise/background-noise.json
          #   min_snr_db: 0.0
          #   max_snr_db: 30.0
          #   probability: 0.2

        feature_augment:
          freq_masking:
            _target_: ezspeech.modules.dataset.augment.FrequencyMasking
            freq_masks: 1
            freq_width: 27

          time_masking:
            _target_: ezspeech.modules.dataset.augment.TimeMasking
            time_masks: 10
            time_width: 0.05


    val_ds:
      _target_: ezspeech.modules.dataset.dataset.SpeechRecognitionDataset
      filepaths:
        - /home4/khanhnd/Ezspeech/data/test.jsonl

      vocab: *vocab
    loaders:
      batch_size: 2
      num_workers: 8
      pin_memory: false

  model:
    # variables for TDT configs.
    tdt_durations: [0, 1, 2, 3, 4]
    num_tdt_durations: 5
    d_model: &d_model 512
    vocab_size: &vocab_size 1825
    # pretrained_weights: /data/datbt7/checkpoints/asr-offline-60000h.ckpt


    encoder:
      _target_: ezspeech.modules.encoder.conformer.ConformerEncoder
      feat_in: 128
      n_layers: 12
      d_model: *d_model
      subsampling_factor: 4
      subsampling_conv_channels: 256

      ff_expansion_factor: 4
      self_attention_model: rel_pos
      n_heads: 8
      att_context_size: [-1, -1]
      att_context_style: regular
      untie_biases: true
      pos_emb_max_len: 5000
      conv_kernel_size: 9
      conv_norm_type: batch_norm
      conv_context_size: null
      dropout: 0.1
      dropout_pre_encoder: 0.1
      dropout_emb: 0.0
      dropout_att: 0.1
      stochastic_depth_drop_prob: 0.0
      stochastic_depth_mode: linear
      stochastic_depth_start_layer: 1
    decoder:
      _target_: ezspeech.modules.decoder.decoder.CTCDecoder
      input_dim: *d_model
      hidden_dim: *d_model
      output_dim: *vocab_size



    predictor:
      _target_: ezspeech.modules.decoder.rnnt.rnnt.RNNTDecoder
      vocab_size: *vocab_size
      normalization_mode: null # Currently only null is supported for export.
      random_state_sampling: false # Random state sampling: https://arxiv.org/pdf/1910.11455.pdf
      blank_as_pad: true # This flag must be set in order to support exporting of RNNT models + efficient inference.

      prednet:
        pred_hidden: *d_model
        pred_rnn_layers: 1
        t_max: null
        dropout: 0.2

    joint:
      _target_: ezspeech.modules.decoder.rnnt.rnnt.RNNTJoint
      num_classes: *vocab_size
      log_softmax: null  # 'null' would set it automatically according to CPU/GPU device
      preserve_memory: false  # dramatically slows down training, but might preserve some memory

      # Fuses the computation of prediction net + joint net + loss + WER calculation
      # to be run on sub-batches of size `fused_batch_size`.
      # When this flag is set to true, consider the `batch_size` of *_ds to be just `encoder` batch size.
      # `fused_batch_size` is the actual batch size of the prediction net, joint net and transducer loss.
      # Using small values here will preserve a lot of memory during training, but will make training slower as well.
      # An optimal ratio of fused_batch_size : *_ds.batch_size is 1:1.
      # However, to preserve memory, this ratio can be 1:8 or even 1:16.
      # Extreme case of 1:B (i.e. fused_batch_size=1) should be avoided as training speed would be very slow.
      fuse_loss_wer: true
      fused_batch_size: 1

      jointnet:
        encoder_hidden: *d_model
        pred_hidden: ${task.model.predictor.prednet.pred_hidden}
        joint_hidden: *d_model
        activation: "relu"
        dropout: 0.2
      num_extra_outputs: ${task.model.num_tdt_durations}

     
      
    loss:
      ctc_loss:
        _target_: ezspeech.modules.losses.ctc.CTCLoss
        blank_idx: 0
        reduction: mean
        zero_infinity: False
      rnnt_loss:
        _target_: ezspeech.modules.losses.rnnt.TDTLoss
        blank_idx: 0
        reduction: mean
        # FastEmit regularization: https://arxiv.org/abs/2010.11148
        # You may enable FastEmit to reduce the latency of the model for streaming
        fastemit_lambda: 0.001  # Recommended values to be in range [1e-4, 1e-2], 0.001 is a good start.
        clamp: -1.0  # if > 0, applies gradient clamping in range [-clamp, clamp] for the joint tensor only.

        # refer to https://arxiv.org/abs/2304.06795 for the meaning of the following three configs.
        durations: ${task.model.tdt_durations}
        sigma: 0.05 # hyper-param for under-normalization.
        omega: 0.1 # weight for regular RNN-T loss.
    metric:
      decoding: self.decoding,
      batch_dim_index: self.wer.batch_dim_index,
      use_cer: self.wer.use_cer,
      log_prediction: self.wer.log_prediction,
      dist_sync_on_step: True,
    
    optimizer:
      lr: 1
      betas: [0.9, 0.999]
      weight_decay: 1e-2
      eps: 1e-9
      foreach: False
      fused: True

    scheduler:
      d_model: *d_model
      warmup_steps: 10000
  
callbacks:
  lr:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor

  swa:
    _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
    swa_lrs: 1e-6
    swa_epoch_start: 0.8
    annealing_epochs: 1

  cb:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: step
    save_top_k: 5
    save_last: True
    filename: "{epoch}-{val_ctc_loss:.5f}-{step}"
    every_n_epochs: 1
    mode: max

loggers:
  tb:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ../lightning_logs/oov
    name: null
    version: testcode1

    default_hp_metric: false

trainer:
  max_epochs: 200
  strategy: ddp
  accelerator: gpu
  devices: [0]
  accumulate_grad_batches: 1
  # detect_anomaly: True
  # precision: 16
  val_check_interval: 0.5