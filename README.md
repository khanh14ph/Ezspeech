# EzSpeech

A modern, easy-to-use speech recognition toolkit built on PyTorch Lightning. EzSpeech provides state-of-the-art ASR models with clean APIs for training and evaluation.

## ğŸš€ Features

- **Multiple Model Architectures**: CTC and Transducer-based ASR models
- **Advanced Encoders**: Conformer, Fast Conformer architectures
- **Easy Training**: Simplified training workflows with Hydra configuration
- **Comprehensive Evaluation**: Detailed metrics and error analysis
- **GPU Optimization**: Efficient inference and training
- **Pre-trained Models**: Support for transfer learning

## ğŸ“¦ Installation

### Quick Install

```bash
git clone https://github.com/khanh14ph/EzSpeech.git
cd EzSpeech
pip install -e .
```

### Development Install

```bash
git clone https://github.com/khanh14ph/EzSpeech.git
cd EzSpeech
pip install -e ".[dev]"
```

## ğŸš‚ Training

### Quick Start

```bash
# Train CTC model with grapheme+phoneme
python scripts/train.py --config-name=ctc_sc

# Train standard CTC model
python scripts/train.py --config-name=ctc

# Train Transducer model
python scripts/train.py --config-name=asr
```

### Custom Configuration

Create your own config file in `config/` directory:

```yaml
# config/my_config.yaml
dataset:
  train_ds:
    filepaths:
      - /path/to/train.jsonl
    data_dir: /path/to/audio/
  val_ds:
    filepaths:
      - /path/to/val.jsonl
    data_dir: /path/to/audio/

model:
  d_model: 512
  vocab_size: 1024
  # ... other model parameters

trainer:
  max_epochs: 20
  devices: [0]
  precision: 16
```

Then train with:

```bash
python scripts/train.py --config-name=my_config
```

## ğŸ“Š Evaluation

### Evaluation Script

Use the evaluation utilities in `ezspeech/script/eval.py` for evaluating your models.

### Metrics

EzSpeech provides comprehensive metrics:

- **Word Error Rate (WER)** and **Character Error Rate (CER)**
- **Sentence-level accuracy**
- **Detailed error analysis** (substitutions, insertions, deletions)
- **Length statistics**

## ğŸ¤ Inference

### Inference Script

Use the inference script for transcribing audio files:

```bash
python ezspeech/script/infer.py \
  --checkpoint /path/to/checkpoint.pt \
  --tokenizer /path/to/tokenizer.model \
  --input /path/to/audio.wav
```

For batch processing and detailed usage, refer to the script's help:

```bash
python ezspeech/script/infer.py --help
```

## ğŸš€ Deployment

### Docker Deployment

Test locally with Docker:

```bash
# Build and run with docker-compose
docker-compose up -d

# View logs
docker-compose logs -f
```

For detailed deployment options and configurations, see [DEPLOYMENT.md](DEPLOYMENT.md).

## ğŸ“ Dataset Format

EzSpeech uses JSONL format for datasets:

```json
{"audio_filepath": "/path/to/audio.wav", "text": "transcription text", "duration": 3.2}
{"audio_filepath": "/path/to/audio2.wav", "text": "another transcription", "duration": 4.1}
```

### Required Fields

- `audio_filepath`: Path to audio file (relative to `data_dir` or absolute)
- `text`: Ground truth transcription
- `duration`: Audio duration in seconds (optional but recommended)

### Supported Audio Formats

- WAV, FLAC, MP3, OGG
- Sample rates: 8kHz, 16kHz, 22kHz, 44.1kHz (automatically resampled to 16kHz)
- Mono or stereo (automatically converted to mono)

## ğŸ—ï¸ Project Structure

```
EzSpeech/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ ctc_sc.yaml         # CTC with grapheme+phoneme
â”‚   â”œâ”€â”€ ctc.yaml            # Standard CTC
â”‚   â”œâ”€â”€ asr.yaml            # Transducer model
â”‚   â”œâ”€â”€ streaming.yaml      # Streaming model
â”‚   â”œâ”€â”€ eval.yaml           # Evaluation config
â”‚   â””â”€â”€ test.yaml           # Test configuration
â”œâ”€â”€ scripts/                # Main scripts
â”‚   â”œâ”€â”€ train.py            # Training script
â”‚   â”œâ”€â”€ build_lexicon.py    # Build lexicon
â”‚   â”œâ”€â”€ csv_to_jsonl.py     # Data conversion
â”‚   â””â”€â”€ export.py           # Model export
â”œâ”€â”€ examples/               # Usage examples
â”‚   â”œâ”€â”€ websocket_client.py # WebSocket client example
â”‚   â”œâ”€â”€ evaluation_usage.md # Evaluation examples
â”‚   â””â”€â”€ README.md           # Examples documentation
â”œâ”€â”€ ezspeech/              # Core package
â”‚   â”œâ”€â”€ models/            # Model definitions
â”‚   â”œâ”€â”€ modules/           # Lightning modules
â”‚   â”œâ”€â”€ layers/            # Neural network layers
â”‚   â”œâ”€â”€ script/            # Inference and utility scripts
â”‚   â”‚   â”œâ”€â”€ infer.py                  # Inference script
â”‚   â”‚   â”œâ”€â”€ eval.py                   # Evaluation utilities
â”‚   â”‚   â”œâ”€â”€ train_tokenizer.py        # Tokenizer training
â”‚   â”‚   â””â”€â”€ validate_training.py      # Training validation
â”‚   â””â”€â”€ utils/             # Utilities
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ demo/                  # Demo scripts
â”œâ”€â”€ dockerfile             # Container image
â””â”€â”€ docker-compose.yml     # Local development
```

## ğŸ”§ Configuration

EzSpeech uses [Hydra](https://hydra.cc/) for configuration management. Key configuration sections:

### Dataset Configuration

```yaml
dataset:
  spe_file_grapheme: /path/to/grapheme.model    # SentencePiece model
  spe_file_phoneme: /path/to/phoneme.model      # Optional phoneme model
  train_ds:
    _target_: ezspeech.modules.data.dataset.SpeechRecognitionDataset
    filepaths: [/path/to/train.jsonl]
    data_dir: /path/to/audio/
  train_loader:
    max_batch_duration: 130  # Total audio seconds per batch
    num_bucket: 20           # Bucketing for efficiency
```

### Model Configuration

```yaml
model:
  d_model: 512              # Model dimension
  vocab_size: 1024          # Vocabulary size
  encoder:
    _target_: ezspeech.modules.encoder.conformer_offline.ConformerOfflineEncoder
    n_layers: 12
    d_model: 512
    ff_expansion_factor: 4
  ctc_decoder:
    _target_: ezspeech.modules.decoder.decoder.ConvASRDecoder
    num_classes: ${model.vocab_size}
```

### Training Configuration

```yaml
trainer:
  max_epochs: 20
  devices: [0]              # GPU devices
  precision: 16             # Mixed precision
  strategy: ddp             # Distributed training
  accumulate_grad_batches: 1
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run tests
pytest tests/

# Run with coverage
pytest tests/ --cov=ezspeech --cov-report=html
```

## ğŸ“ˆ Performance Tips

### Training Optimization

1. **Batch Size**: Use `max_batch_duration` instead of fixed batch size
2. **Mixed Precision**: Enable with `trainer.precision=16`
3. **Distributed Training**: Use `trainer.strategy=ddp` for multi-GPU
4. **Bucketing**: Use `num_bucket` for efficient batching

### Inference Optimization

1. **Batch Processing**: Process multiple files efficiently
2. **GPU Utilization**: Ensure optimal GPU usage during inference
3. **TorchScript**: Export models for faster inference
4. **ONNX**: Use ONNX runtime for deployment
5. **Quantization**: Apply post-training quantization

## ğŸ¤ Contributing

We welcome contributions!

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ™ Acknowledgments

- PyTorch Lightning team for the excellent framework
- Hydra team for configuration management
- The speech recognition research community

## ğŸ¯ Quick Links

- **[ğŸ“š Online Documentation](https://khanh14ph.github.io/Ezspeech)** - Interactive guides and tutorials
- **[ğŸ³ Deployment Options](DEPLOYMENT.md)** - Deployment methods and configurations
- **[ğŸ’¡ Examples](examples/)** - Code samples and usage examples

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/khanh14ph/EzSpeech/issues)
- **Discussions**: [GitHub Discussions](https://github.com/khanh14ph/EzSpeech/discussions)
- **Documentation**: [Online Docs](https://khanh14ph.github.io/Ezspeech)
